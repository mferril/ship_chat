# SHiP chat â€” Private Retrieval-Augmented Chatbot

`ship_chat` is a two-part system:

| Role | Where it runs | What it does | Frequency |
|------|---------------|--------------|-----------|
| **Builder** | secure internal server | â€¢ extracts text from PDFs<br>â€¢ chunks & embeds once<br>â€¢ writes a **read-only** Chroma vector DB to `data/chroma/` | **one-time** (or whenever PDFs change) |
| **User** | any laptop | â€¢ loads the pre-built DB<br>â€¢ retrieves top-k chunks for each query<br>â€¢ feeds them to a local Ollama LLM | interactive, ad-hoc |

Everything stays on-prem; no API calls, no telemetry.

---

## Quick start (Builder)

```bash
git clone GitHub_PLACEHOLDER
cd ship_chat
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Download models once
./scripts/pull_models.sh          # huggingface + ollama

# Put your PDFs into data/pdfs/
ship_chat build --cfg cfg/builder.yaml
# ğŸ‘‰  data/chroma/ is created â€“ copy/mount it read-only for users
```

## Quick start (User)
```bash
# assume data/chroma/ + cfg/user.yaml are already present
ollama serve &                    # local LLM API
ship_chat chat --cfg cfg/user.yaml
> What is our neutrino DIS background?
```

## Directory Layout
```bash
ship_chat/
â”œâ”€â”€ cfg/               # YAML configs (builder vs user)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/          # raw source documents (builder only)
â”‚   â””â”€â”€ chroma/        # persisted vector DB (read-only for users)
â”œâ”€â”€ ship_chat/         # importable Python package
â”‚   â”œâ”€â”€ ingest.py      # PDF to text
â”‚   â”œâ”€â”€ chunk.py       # text to chunks
â”‚   â”œâ”€â”€ embed.py       # chunks to vectors
â”‚   â”œâ”€â”€ vector.py      # Chroma helpers
â”‚   â”œâ”€â”€ rag.py         # retrieval-augmented generation
â”‚   â””â”€â”€ cli.py         # â€œship_chat build | chatâ€
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ pull_models.sh # pulls HuggingFace & Ollama models locally
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

## Security Notes
* No telemetry â€“ Chroma is initialised with anonymized_telemetry=False.
* No cloud calls â€“ models are downloaded once with scripts/pull_models.sh and then loaded from disk.
* Ollama listens on localhost only.